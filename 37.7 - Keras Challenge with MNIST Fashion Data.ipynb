{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 7 - Challenge\n",
    "Now take your Keras skills and go build another neural network. Pick your data set, but it should be one of abstract types, possibly even nonnumeric, and use Keras to make five implementations of your network. Compare them both in computational complexity as well as in accuracy and given that tradeoff decide which one you like best.\n",
    "\n",
    "Your dataset should be sufficiently large for a neural network to perform well (samples should really be in the thousands here) and try to pick something that takes advantage of neural networks’ ability to have both feature extraction and supervised capabilities, so don’t pick something with an easy to consume list of features already generated for you (though neural networks can still be useful in those contexts).\n",
    "\n",
    "Note that if you want to use an unprocessed image dataset, scikit-image is a useful package for converting to importable numerics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\M246047\\\\Documents\\\\Python')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from keras.datasets import mnist\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('Fashion-MNIST', version=1, cache=True)\n",
    "\n",
    "indices = np.random.choice(70000,10000)\n",
    "X = mnist.data[indices] / 255.0\n",
    "y = mnist.target[indices]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model One - Multilayer Perceptron \n",
    "\n",
    "Test loss: 0.08862657889742404 <br>\n",
    "Test accuracy: 0.9746000170707703\n",
    "\n",
    "We're starting out with a very good model - I'm curious to see if others can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Change shape \n",
    "# Note that our images are 28*28 pixels, so in reshaping to arrays we want\n",
    "# 60,000 arrays of length 784, one for each image\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, create 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.4336 - accuracy: 0.8733 - val_loss: 0.1890 - val_accuracy: 0.9435\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2053 - accuracy: 0.9390 - val_loss: 0.1387 - val_accuracy: 0.9585\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.1610 - accuracy: 0.9516 - val_loss: 0.1259 - val_accuracy: 0.9623\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.1365 - accuracy: 0.9583 - val_loss: 0.1099 - val_accuracy: 0.9659\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.0992 - val_accuracy: 0.9707\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.1075 - accuracy: 0.9672 - val_loss: 0.1016 - val_accuracy: 0.9702\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0983 - accuracy: 0.9699 - val_loss: 0.0914 - val_accuracy: 0.9733\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0915 - accuracy: 0.9716 - val_loss: 0.0874 - val_accuracy: 0.9753\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0862 - accuracy: 0.9732 - val_loss: 0.0882 - val_accuracy: 0.9758\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0807 - accuracy: 0.9744 - val_loss: 0.0886 - val_accuracy: 0.9746\n",
      "Test loss: 0.08862657889742404\n",
      "Test accuracy: 0.9746000170707703\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - CNN\n",
    "\n",
    "Test loss: 0.02581354253039426 <br>\n",
    "Test accuracy: 0.9922999739646912\n",
    "\n",
    "This mode performs almost perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 0.2816 - accuracy: 0.9129 - val_loss: 0.0608 - val_accuracy: 0.9802\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0915 - accuracy: 0.9736 - val_loss: 0.0416 - val_accuracy: 0.9858\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0683 - accuracy: 0.9809 - val_loss: 0.0325 - val_accuracy: 0.9889\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0560 - accuracy: 0.9836 - val_loss: 0.0308 - val_accuracy: 0.9899\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0475 - accuracy: 0.9861 - val_loss: 0.0319 - val_accuracy: 0.9899\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 0.0429 - accuracy: 0.9869 - val_loss: 0.0287 - val_accuracy: 0.9904\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0382 - accuracy: 0.9886 - val_loss: 0.0314 - val_accuracy: 0.9891\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.0299 - val_accuracy: 0.9904\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0319 - accuracy: 0.9905 - val_loss: 0.0276 - val_accuracy: 0.9912\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0300 - accuracy: 0.9910 - val_loss: 0.0258 - val_accuracy: 0.9923\n",
      "Test loss: 0.02581354253039426\n",
      "Test accuracy: 0.9922999739646912\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - CNN with Fewer Layers and Tanh Activation\n",
    "\n",
    "Test loss: 0.03989340436264174 <br>\n",
    "Test accuracy: 0.9876999855041504\n",
    "\n",
    "This model performs well, but not as well as the model with more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.2307 - accuracy: 0.9300 - val_loss: 0.0924 - val_accuracy: 0.9725\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0863 - accuracy: 0.9747 - val_loss: 0.0617 - val_accuracy: 0.9799\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0661 - accuracy: 0.9799 - val_loss: 0.0508 - val_accuracy: 0.9832\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.0539 - accuracy: 0.9836 - val_loss: 0.0509 - val_accuracy: 0.9831\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.0476 - accuracy: 0.9854 - val_loss: 0.0548 - val_accuracy: 0.9839\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0416 - accuracy: 0.9872 - val_loss: 0.0523 - val_accuracy: 0.9842\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.0383 - accuracy: 0.9883 - val_loss: 0.0531 - val_accuracy: 0.9848\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 103s 2ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 0.0521 - val_accuracy: 0.9841\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0303 - accuracy: 0.9901 - val_loss: 0.0570 - val_accuracy: 0.9837\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0270 - accuracy: 0.9915 - val_loss: 0.0528 - val_accuracy: 0.9857\n",
      "Test loss: 0.052781391357822574\n",
      "Test accuracy: 0.9857000112533569\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='tanh',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 - CNN with More Layers and Relu Activation\n",
    "\n",
    "Test loss: 0.05307199218791486 <br>\n",
    "Test accuracy: 0.9884999990463257\n",
    "\n",
    "This model performs well, but not as well as the model with more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.6255 - accuracy: 0.8009 - val_loss: 0.0937 - val_accuracy: 0.9755\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.2408 - accuracy: 0.9363 - val_loss: 0.0691 - val_accuracy: 0.9839\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1836 - accuracy: 0.9525 - val_loss: 0.0737 - val_accuracy: 0.9833\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1639 - accuracy: 0.9579 - val_loss: 0.0537 - val_accuracy: 0.9871\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1493 - accuracy: 0.9618 - val_loss: 0.0546 - val_accuracy: 0.9862\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1399 - accuracy: 0.9658 - val_loss: 0.0623 - val_accuracy: 0.9857\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1321 - accuracy: 0.9670 - val_loss: 0.0528 - val_accuracy: 0.9880\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1254 - accuracy: 0.9690 - val_loss: 0.0472 - val_accuracy: 0.9898\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1264 - accuracy: 0.9684 - val_loss: 0.0517 - val_accuracy: 0.9883\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 87s 1ms/step - loss: 0.1230 - accuracy: 0.9692 - val_loss: 0.0531 - val_accuracy: 0.9885\n",
      "Test loss: 0.05307199218791486\n",
      "Test accuracy: 0.9884999990463257\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5 - Hierarchical RNN with Three Epochs and 64 Batch Size\n",
    "\n",
    "Test loss: 0.17047725892663002 <br>\n",
    "Test accuracy: 0.9505000114440918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 185s 3ms/step - loss: 0.8975 - accuracy: 0.6965 - val_loss: 0.5738 - val_accuracy: 0.8064\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 185s 3ms/step - loss: 0.3634 - accuracy: 0.8860 - val_loss: 0.2406 - val_accuracy: 0.9251\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 186s 3ms/step - loss: 0.2138 - accuracy: 0.9356 - val_loss: 0.1654 - val_accuracy: 0.9500\n",
      "Test loss: 0.1654222881436348\n",
      "Test accuracy: 0.949999988079071\n"
     ]
    }
   ],
   "source": [
    "# Training parameters.\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# The data, shuffled and split between train and test sets.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6 - Hierarchical RNN with Six Epochs and 32 Batch Size\n",
    "\n",
    "Test loss: 0.07090047200620174 <br>\n",
    "Test accuracy: 0.9779999852180481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.7853 - accuracy: 0.7275 - val_loss: 0.3489 - val_accuracy: 0.8891\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.2721 - accuracy: 0.9169 - val_loss: 0.1645 - val_accuracy: 0.9490\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 198s 3ms/step - loss: 0.1765 - accuracy: 0.9463 - val_loss: 0.1329 - val_accuracy: 0.9580\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 194s 3ms/step - loss: 0.1332 - accuracy: 0.9599 - val_loss: 0.1017 - val_accuracy: 0.9693\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.1092 - accuracy: 0.9668 - val_loss: 0.0971 - val_accuracy: 0.9694\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 194s 3ms/step - loss: 0.0934 - accuracy: 0.9712 - val_loss: 0.0709 - val_accuracy: 0.9780\n",
      "Test loss: 0.07090047200620174\n",
      "Test accuracy: 0.9779999852180481\n"
     ]
    }
   ],
   "source": [
    "# Training parameters.\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 6\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# The data, shuffled and split between train and test sets.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Test Loss | Test Accuracy | \n",
    "|------|------|------|\n",
    "| Multilayer Perceptron with Five Layers | 0.0886 | 0.9746 |\n",
    "| CNN with Eight Layers and Relu Activation | 0.0258 | 0.9923 |\n",
    "| CNN with Six Layers and Tanh Activation | 0.0528 | 0.9857 |\n",
    "| CNN with Twelve Layers and Relu Activation | 0.0531 | 0.9885 |\n",
    "| Hierarchical RNN Three Epochs and 64 Batch Size |  0.1654 | 0.9500 |\n",
    "| Hierarchical RNN with Six Epochs and 32 Batch Size | 0.07090 | 0.9780 |\n",
    "\n",
    "\n",
    "The CNN model with Relu activation and eight layers performed best with the lowest test lost and highest accuracy. I'm curious that the same model with twelve layers didn't perform better - perhaps this is because of overfitting?\n",
    "\n",
    "The Hierarchical RNN model improved with a larger number of epochs and smaller batch size. My guess is that the batch size didn't matter as much and the improvement is due to the increase in epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
