{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a multi-layer perceptron neural network model to predict on a labeled dataset of your choosing. Compare this model to either a boosted tree or a random forest model and describe the relative tradeoffs between complexity and accuracy. Be sure to vary the hyperparameters of your MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - lime\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2018.4.16  |                0         176 KB  conda-forge\n",
      "    certifi-2019.11.28         |           py37_0         157 KB\n",
      "    lime-0.1.1.37              |             py_0         234 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         567 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  lime               conda-forge/noarch::lime-0.1.1.37-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  certifi                                  2019.9.11-py37_0 --> 2019.11.28-py37_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2019.10.16~ --> conda-forge::ca-certificates-2018.4.16-0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ca-certificates-2018 | 176 KB    |            |   0% \n",
      "ca-certificates-2018 | 176 KB    | 9          |   9% \n",
      "ca-certificates-2018 | 176 KB    | ########## | 100% \n",
      "\n",
      "lime-0.1.1.37        | 234 KB    |            |   0% \n",
      "lime-0.1.1.37        | 234 KB    | ########## | 100% \n",
      "\n",
      "certifi-2019.11.28   | 157 KB    |            |   0% \n",
      "certifi-2019.11.28   | 157 KB    | #          |  10% \n",
      "certifi-2019.11.28   | 157 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\M246047\\\\Documents\\\\Python')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "%matplotlib inline\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Columns\n",
      "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
      "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
      "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst', 'fractal_dimension_worst'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Breast Cancer Columns')\n",
    "br = pd.read_csv('breast_cancer.csv')\n",
    "br = pd.DataFrame(br)\n",
    "print(br.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      "id                         569 non-null int64\n",
      "diagnosis                  569 non-null object\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 140.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "B    357\n",
       "M    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.info()\n",
    "br.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = br.diagnosis\n",
    "y_dum = br['diagnosis'].apply(lambda x: 1 if 'M' else 0)\n",
    "X = br.drop(columns=['diagnosis', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Models\n",
    "\n",
    "## Multiple Layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Score:  0.8541300527240774\n",
      "Cross Validation Score:  [0.85964912 0.9122807  0.92982456 0.94736842 0.92982456 0.92982456\n",
      " 0.98245614 0.68421053 0.92982456 0.91071429]\n",
      "Mean Cross Validation Score:  0.9015977443609022\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,))\n",
    "mlp.fit(X, y)\n",
    "print('MLP Score: ', mlp.score(X, y))\n",
    "      \n",
    "cross_val = cross_val_score(mlp, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Score:  0.9367311072056239\n",
      "Cross Validation Score:  [0.94736842 0.85964912 0.92982456 0.94736842 0.92982456 0.92982456\n",
      " 0.9122807  0.92982456 0.89473684 0.98214286]\n",
      "Mean Cross Validation Score:  0.9262844611528823\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,))\n",
    "mlp.fit(X, y)\n",
    "print('MLP Score: ', mlp.score(X, y))\n",
    "      \n",
    "cross_val = cross_val_score(mlp, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (569,30) into shape (569)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-3e664cfedde5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\lime\\lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;31m# Preventative code: if sparse, convert to csr format if not in csr format already\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0mdata_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_row\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__data_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Note in sparse case we don't subtract mean since data would become dense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\lime\\lime_tabular.py\u001b[0m in \u001b[0;36m__data_inverse\u001b[1;34m(self, data_row, num_samples)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m             \u001b[0mfirst_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscretizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscretize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_row\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m         \u001b[0minverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (569,30) into shape (569)"
     ]
    }
   ],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X.values)\n",
    "\n",
    "def prob(data):\n",
    "    return np.array(list(zip(1-model.predict(X),model.predict(X))))\n",
    "\n",
    "i = 1\n",
    "exp = explainer.explain_instance(X.values, prob, num_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Score:  0.9103690685413005\n",
      "Cross Validation Score:  [0.92982456 0.85964912 0.9122807  0.94736842 0.92982456 0.87719298\n",
      " 0.9122807  0.92982456 0.92982456 0.91071429]\n",
      "Mean Cross Validation Score:  0.9138784461152882\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100))\n",
    "mlp.fit(X, y)\n",
    "print('MLP Score: ', mlp.score(X, y))\n",
    "\n",
    "cross_val = cross_val_score(mlp, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Score:  0.9244288224956063\n",
      "Cross Validation Score:  [0.92982456 0.94736842 0.92982456 0.94736842 0.9122807  0.9122807\n",
      " 0.9122807  0.89473684 0.84210526 0.96428571]\n",
      "Mean Cross Validation Score:  0.919235588972431\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100))\n",
    "mlp.fit(X, y)\n",
    "print('MLP Score: ', mlp.score(X, y))\n",
    "      \n",
    "cross_val = cross_val_score(mlp, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Score:  0.9349736379613357\n",
      "Cross Validation Score:  [0.92982456 0.85964912 0.92982456 0.94736842 0.94736842 0.92982456\n",
      " 0.94736842 0.92982456 0.9122807  0.875     ]\n",
      "Mean Cross Validation Score:  0.9208333333333334\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(50, 50))\n",
    "mlp.fit(X, y)\n",
    "print('MLP Score: ', mlp.score(X, y))\n",
    "      \n",
    "cross_val = cross_val_score(mlp, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Score:  0.6274165202108963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  [0.92982456 0.9122807  0.9122807  0.94736842 0.87719298 0.89473684\n",
      " 0.98245614 0.9122807  0.89473684 0.91071429]\n",
      "Mean Cross Validation Score:  0.9173872180451129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10))\n",
    "mlp.fit(X, y)\n",
    "print('MLP Score: ', mlp.score(X, y))\n",
    "      \n",
    "cross_val = cross_val_score(mlp, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for the best Multiple Layer Perceptron Classifier:  MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
      "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(20, 20), learning_rate='adaptive',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=300,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp_gsc = GridSearchCV(\n",
    "        estimator=MLPClassifier(),\n",
    "        param_grid={\n",
    "            'hidden_layer_sizes': [(20, 20), (10), (50, 10)],\n",
    "            'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            'max_iter': [100, 200, 300]\n",
    "        },\n",
    "        cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "mlp_gsc.fit(X, y)\n",
    "best_params = mlp_gsc.best_params_\n",
    "# svr_gsc.best_estimator_\n",
    "best_mlp = MLPClassifier(hidden_layer_sizes=best_params['hidden_layer_sizes'], activation=best_params[\"activation\"],\n",
    "                                         learning_rate=best_params[\"learning_rate\"], max_iter=best_params[\"max_iter\"],\n",
    "                                         verbose=False)\n",
    "\n",
    "print('Parameters for the best Multiple Layer Perceptron Classifier: ', best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Score:  0.9560632688927944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\M246047\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  [0.92982456 0.92982456 0.9122807  0.92982456 0.98245614 0.92982456\n",
      " 0.96491228 0.9122807  0.89473684 0.98214286]\n",
      "Mean Cross Validation Score:  0.9368107769423559\n"
     ]
    }
   ],
   "source": [
    "best_mlp.fit(X, y)\n",
    "print('MLP Score: ', best_mlp.score(X, y))\n",
    "      \n",
    "cross_val = cross_val_score(best_mlp, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These features were obtained from the feature importance attribute in the random forests model.\n",
    "X_top_features = X[['concave points_mean', 'radius_worst', 'perimeter_worst', 'concave points_worst']]\n",
    "\n",
    "best_mlp.fit(X_top_features, y)\n",
    "print('MLP Score: ', best_mlp.score(X_top_features, y))\n",
    "      \n",
    "cross_val = cross_val_score(best_mlp, X_top_features, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      "id                         569 non-null int64\n",
      "diagnosis                  569 non-null int64\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "dtypes: float64(30), int64(2)\n",
      "memory usage: 142.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Updating the target to a numerical variable.\n",
    "br['diagnosis'] = br['diagnosis'].apply(lambda x: 1 if 'M' else 0)\n",
    "br.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  [0.98245614 0.89473684 0.94736842 0.94736842 1.         0.98245614\n",
      " 0.92982456 0.98245614 0.94736842 1.        ]\n",
      "Mean Cross Validation Score:  0.9614035087719298\n",
      "\n",
      " Feature Importances:  [0.02429854 0.01419575 0.0539929  0.04540104 0.00795465 0.00345576\n",
      " 0.07186229 0.08852713 0.00558817 0.0021412  0.01900904 0.00490146\n",
      " 0.01459376 0.0394196  0.00374584 0.00366822 0.00424091 0.00508478\n",
      " 0.00371126 0.0059118  0.12198315 0.0191726  0.17398717 0.07498058\n",
      " 0.01269208 0.01023589 0.03039391 0.11858904 0.00796357 0.0082979 ]\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "rfc.fit(X, y)\n",
    "# Train the classifier\n",
    "cross_val = cross_val_score(rfc, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())\n",
    "print('\\n Feature Importances: ', rfc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  [0.98245614 0.89473684 0.94736842 0.94736842 1.         0.98245614\n",
      " 0.92982456 0.98245614 0.94736842 1.        ]\n",
      "Mean Cross Validation Score:  0.9614035087719298\n",
      "\n",
      " Feature Importances:  [0.02429854 0.01419575 0.0539929  0.04540104 0.00795465 0.00345576\n",
      " 0.07186229 0.08852713 0.00558817 0.0021412  0.01900904 0.00490146\n",
      " 0.01459376 0.0394196  0.00374584 0.00366822 0.00424091 0.00508478\n",
      " 0.00371126 0.0059118  0.12198315 0.0191726  0.17398717 0.07498058\n",
      " 0.01269208 0.01023589 0.03039391 0.11858904 0.00796357 0.0082979 ]\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "rfc.fit(X, y)\n",
    "# Train the classifier\n",
    "cross_val = cross_val_score(rfc, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())\n",
    "print('\\n Feature Importances: ', rfc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.98245614 0.9122807  0.92982456 0.94736842 1.         0.98245614\n",
      " 0.98245614 0.98245614 0.94736842 0.94642857]\n",
      "Mean Cross Validation Score:  0.9613095238095237\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier with all variables with >0.05 feature importance. \n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "X_top_features = X[['radius_mean', 'smoothness_mean', 'compactness_mean', 'symmetry_se', 'radius_worst', 'texture_worst', 'smoothness_worst']]\n",
    "rfc.fit(X_top_features, y)\n",
    "# Train the classifier\n",
    "cross_val = cross_val_score(rfc, X_top_features, y, cv=10)\n",
    "print('Cross Validation Scores: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   58.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for the best Random Forest Classifier:  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features=2,\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=2, min_samples_split=12,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=False, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  2.0min finished\n"
     ]
    }
   ],
   "source": [
    "rfc_gsc = GridSearchCV(\n",
    "        estimator=ensemble.RandomForestClassifier(),\n",
    "        param_grid={\n",
    "            'max_depth': [50, 100],\n",
    "            'max_features': [2, 3, 4],\n",
    "            'min_samples_leaf': [2, 3, 4],\n",
    "            'min_samples_split': [8, 10, 12],\n",
    "            'n_estimators': [50, 100, 500]\n",
    "        },\n",
    "        cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "rfc_gsc.fit(X_top_features, y)\n",
    "best_params = rfc_gsc.best_params_\n",
    "# svr_gsc.best_estimator_\n",
    "best_rfc = ensemble.RandomForestClassifier(max_depth=best_params['max_depth'], max_features=best_params[\"max_features\"],\n",
    "                                         min_samples_leaf=best_params[\"min_samples_leaf\"], min_samples_split=best_params[\"min_samples_split\"],\n",
    "                                         n_estimators=best_params['n_estimators'],verbose=False)\n",
    "\n",
    "print('Parameters for the best Random Forest Classifier: ', best_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.98245614 0.89473684 0.92982456 0.96491228 1.         0.96491228\n",
      " 0.96491228 0.96491228 0.92982456 0.96428571]\n",
      "Mean Cross Validation Score:  0.9560776942355889\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=50, max_features=2, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=4, min_samples_split=8,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=False, warm_start=False)\n",
    "rfc.fit(X_top_features, y)\n",
    "# Train the classifier\n",
    "cross_val = cross_val_score(rfc, X_top_features, y, cv=10)\n",
    "print('Cross Validation Scores: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  [0.98245614 0.89473684 0.92982456 0.94736842 0.98245614 0.96491228\n",
      " 0.94736842 0.98245614 0.96491228 1.        ]\n",
      "Mean Cross Validation Score:  0.9596491228070176\n",
      "\n",
      " Feature Importances:  [2.03687402e-04 2.56235944e-02 4.40868303e-04 1.10107092e-03\n",
      " 2.33500093e-05 1.90165830e-03 1.64345099e-04 1.27662646e-01\n",
      " 3.39281262e-04 3.49498483e-04 4.79348111e-03 3.47169417e-03\n",
      " 1.12427183e-03 8.35125203e-03 6.81848816e-04 8.25966536e-03\n",
      " 1.05058392e-02 4.34800595e-03 1.20227781e-03 1.07019007e-03\n",
      " 4.40493319e-01 3.67372245e-02 1.51223505e-01 2.98214765e-02\n",
      " 7.39031161e-03 9.11313135e-04 1.16460252e-02 1.16417839e-01\n",
      " 2.68425524e-03 1.05620442e-03]\n"
     ]
    }
   ],
   "source": [
    "gbc = ensemble.GradientBoostingClassifier(n_estimators=100, random_state=0)\n",
    "gbc.fit(X, y)\n",
    "# Train the classifier\n",
    "cross_val = cross_val_score(gbc, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())\n",
    "print('\\n Feature Importances: ', gbc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  [0.98245614 0.89473684 0.92982456 0.94736842 0.98245614 0.96491228\n",
      " 0.94736842 0.98245614 0.96491228 1.        ]\n",
      "Mean Cross Validation Score:  0.9596491228070176\n",
      "\n",
      " Feature Importances:  [2.03687402e-04 2.56235944e-02 4.40868303e-04 1.10107092e-03\n",
      " 2.33500093e-05 1.90165830e-03 1.64345099e-04 1.27662646e-01\n",
      " 3.39281262e-04 3.49498483e-04 4.79348111e-03 3.47169417e-03\n",
      " 1.12427183e-03 8.35125203e-03 6.81848816e-04 8.25966536e-03\n",
      " 1.05058392e-02 4.34800595e-03 1.20227781e-03 1.07019007e-03\n",
      " 4.40493319e-01 3.67372245e-02 1.51223505e-01 2.98214765e-02\n",
      " 7.39031161e-03 9.11313135e-04 1.16460252e-02 1.16417839e-01\n",
      " 2.68425524e-03 1.05620442e-03]\n"
     ]
    }
   ],
   "source": [
    "gbc = ensemble.GradientBoostingClassifier(n_estimators=100, random_state=0)\n",
    "gbc.fit(X, y)\n",
    "# Train the classifier\n",
    "cross_val = cross_val_score(gbc, X, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())\n",
    "print('\\n Feature Importances: ', gbc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  [0.96491228 0.87719298 0.89473684 0.92982456 0.98245614 0.96491228\n",
      " 0.92982456 0.94736842 0.87719298 0.94642857]\n",
      "Mean Cross Validation Score:  0.931484962406015\n",
      "\n",
      " Feature Importances:  [0.0698127  0.3974149  0.29454461 0.2382278 ]\n"
     ]
    }
   ],
   "source": [
    "# Using features of >0.1 importance.\n",
    "gbc = ensemble.GradientBoostingClassifier(n_estimators=100, random_state=0)\n",
    "X_top_features = X[['concave points_mean', 'radius_worst', 'perimeter_worst', 'concave points_worst']]\n",
    "gbc.fit(X_top_features, y)\n",
    "# Train the classifier\n",
    "cross_val = cross_val_score(gbc, X_top_features, y, cv=10)\n",
    "print('Cross Validation Score: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())\n",
    "print('\\n Feature Importances: ', gbc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for the best Random Forest Classifier:  GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='exponential', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=4, min_samples_split=8,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=False,\n",
      "                           warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "gbc_gsc = GridSearchCV(\n",
    "        estimator=ensemble.GradientBoostingClassifier(),\n",
    "        param_grid={\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'learning_rate': [0.01, 0.1, 0.5],\n",
    "            'min_samples_leaf': [2, 3, 4],\n",
    "            'min_samples_split': [2, 4, 8],\n",
    "            'n_estimators': [50, 100, 500]\n",
    "        },\n",
    "        cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "gbc_gsc.fit(X, y)\n",
    "best_params = gbc_gsc.best_params_\n",
    "# svr_gsc.best_estimator_\n",
    "best_gbc = ensemble.GradientBoostingClassifier(loss=best_params[\"loss\"], learning_rate=best_params[\"learning_rate\"],\n",
    "                                         min_samples_leaf=best_params[\"min_samples_leaf\"], min_samples_split=best_params[\"min_samples_split\"],\n",
    "                                         n_estimators=best_params['n_estimators'],verbose=False)\n",
    "\n",
    "print('Parameters for the best Gradient Boosting Classifier: ', best_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.98245614 0.94736842 0.94736842 0.94736842 0.98245614 0.98245614\n",
      " 0.98245614 0.98245614 0.96491228 0.98214286]\n",
      "Mean Cross Validation Score:  0.9701441102756891\n"
     ]
    }
   ],
   "source": [
    "best_gbc.fit(X, y)\n",
    "# Train the classifier\n",
    "cross_val = cross_val_score(best_gbc, X, y, cv=10)\n",
    "print('Cross Validation Scores: ', cross_val)\n",
    "print('Mean Cross Validation Score: ', cross_val.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a multi-layer perceptron neural network model to predict on a labeled dataset of your choosing. Compare this model to either a boosted tree or a random forest model and describe the relative tradeoffs between complexity and accuracy. Be sure to vary the hyperparameters of your MLP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons\n",
    "\n",
    "MLP: These models were quite a bit slower than the random forest and gradient boosting models. They were in the low 0.9 range before using GridSearch, but hit a high MLP score of 0.956 and mean cross validation score of 0.937 when I used the optimal parameters obtained. While these are both high scores, the other models fared better. I attempted to use the LIME library for information on the features, but was unsuccessful. A final downside of MLPs is that each model runs differently every time it's run - for example, I ran the model twice with the same parameters and received different MLP and cross validation scores.\n",
    "\n",
    "Random Forests and Gradient Boosting: These models were fast, and I was able to use a feature importance attribute to reveal which variables had the highest impacts on the models, giving me more insight into how predictions are made. Additionally, they had higher cross validation means, with random forests hitting a high of 0.961 and graddient boosting 0.970.\n",
    "\n",
    "As of this lesson, I prefer models like Random Forests and Gradient Boosting. In addition to being more accurate, they offer more insight into what's going on inside the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
