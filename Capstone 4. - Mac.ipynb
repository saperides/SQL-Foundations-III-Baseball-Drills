{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "For my final capstone, I will create a model that processes a piece of an xray of breast tissue and classifies whether or not image contains cancerous tissue. Breast cancer is the most common form of cancer in women, contributing just over a quarter of diagnoses, so having the ability to improve the timeline and accuracy of a diagnosis would greatly improve the disease.\n",
    "\n",
    "My solution will be valuable because it will capture this information more quickly and accurately than a person could. It would be a model used by hospitals and other healthcare organizations to identify the presence of cancerous tissue. This model would be able to see abnormalities at a much earlier stage which would allow treatment to commence earlier and have a higher success rate. Ideally, the model could be adjusted for other types of tumor or disease classifications.\n",
    "\n",
    "I will use the [Breast Histopathology Images dataset](https://www.kaggle.com/paultimothymooney/breast-histopathology-images) from the National Library of Medicine. There are 277,524 pieces of 162 images scanned which should be plenty of data to learn from. The data is imbalanced - there is a 34/66 split between the classes. To handle this, I will likely choose a random subset of the images marked IDC negative to ensure I have a balanced dataset.\n",
    "\n",
    "Because the data is images, I don't believe it doesn't to be cleaned and prepped - it will simply be imported into Python and converted into a dataframe in which each column represents a pixel. I will then use dimensionality reduction and unsupervised learning to obtain in-depth knowledge about the information the data provides and visualize any patterns or clusters the information may fall into. I will need to do some research on how to use supervised learning on image processing/classification as it is not yet clear to me how to do so. Finally, I will use neural networks, and CNN in particular, to classify the scans and create a model that can take an un-labeled scan and report whether or not cancerous tissue is present.\n",
    "\n",
    "I anticipate importing and processing the images as well as tuning the parameters in the neural networks to be the biggest challenges I'll face. I plan on spending the majority of my time on these two items. For the most part, I should be able to recycle code from previous projects which should expedite my prepping and modeling. I'll do additional research on how best to use neural networks for image classification in an effort to obtain the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    outPath = '/Users/sophiaperides/Desktop/Thinkful/chest_xrays_folder'\n",
    "    path = '/Users/sophiaperides/Desktop/Thinkful/chest_xrays_folder'\n",
    "\n",
    "    # iterate through the names of contents of the folder\n",
    "    for image_path in os.listdir(path):\n",
    "\n",
    "        # create the full input path and read the file\n",
    "        input_path = os.path.join(path, image_path)\n",
    "        image_to_save = ndimage.imread(input_path)\n",
    "\n",
    "        # rotate the image\n",
    "        rotated = ndimage.rotate(image_to_rotate, 45)\n",
    "\n",
    "        # create full output path, 'example.jpg' \n",
    "        # becomes 'rotate_example.jpg', save the file to disk\n",
    "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
    "        misc.imsave(fullpath, rotated)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/sophiaperides/Desktop/Thinkful')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "\n",
    "# Importing the files\n",
    "from zipfile import ZipFile\n",
    "import cv2\n",
    "from glob import glob\n",
    "from random import sample\n",
    "\n",
    "# Preprocessing/Cleaning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Import various componenets for model building\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Dimensionality Reduction & Unsupervised Learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import LocallyLinearEmbedding, TSNE\n",
    "import umap\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# import hdbscan\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import linkage as lnkg\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.preprocessing.image import load_img, ImageDataGenerator, img_to_array\n",
    "# Data Augmentation\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Prepping the Data\n",
    "I gravely underestimated how difficult this task would be. I had an initial folder for the data which contained folders for each patient. Each of these folders contained two folders labeled 0 and 1 which contained images with or without cancerous tissue, depending on the folder. At first, I tried to iterate through each patient folder to grab the 0 and 1 folders within, and iterate through each of those folder to grab each image to import it, but was unable to do so. In the end I mapped to the initial folder and iterated through each patient folder to create arrays containing the pixel data for each image. Using the length of these arrays, I was able to create an array with the target values, with 0 for a scan without cancerous tissue and 1 for a scan with.\n",
    "\n",
    "# Importing the Images by Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 768 images\n",
      "The type of the data is <class 'numpy.ndarray'>\n",
      "The shape of the data is (768, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "images_0_8863 = []\n",
    "for root, dirs, files in os.walk('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8863/0/'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8863/0/'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(1, 50, 50)\n",
    "            images_0_8863.append(image_data)\n",
    "            \n",
    "images_0_8863 = np.asarray(images_0_8863)    \n",
    "print('There are {} images'.format(len(images_0_8863)))\n",
    "print('The type of the data is', type(images_0_8863))\n",
    "print('The shape of the data is', images_0_8863.shape)\n",
    "images_0_8863_class = np.zeros((len(images_0_8863),), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 207 images\n",
      "The type of the data is <class 'numpy.ndarray'>\n",
      "The shape of the data is (207, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of image names\n",
    "images_1_8863 = []\n",
    "for root, dirs, files in os.walk('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8863/1/'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8863/1/'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(2500,)\n",
    "            images_1_8863.append(image_data)\n",
    "            \n",
    "images_1_8863 = np.asarray(images_1_8863)\n",
    "print('There are {} images'.format(len(images_1_8863)))\n",
    "print('The type of the data is', type(images_1_8863))\n",
    "print('The shape of the data is', images_1_8863.shape)\n",
    "images_1_8863_class = np.ones((len(images_1_8863),), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_0_8864 = []\n",
    "for root, dirs, files in os.walk('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8864/0/'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8864/0/'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(2500,)\n",
    "            images_0_8864.append(image_data)\n",
    "            \n",
    "images_0_8864 = np.asarray(images_0_8864)    \n",
    "print('There are {} images'.format(len(images_0_8864)))\n",
    "print('The type of the data is', type(images_0_8864))\n",
    "print('The shape of the data is', images_0_8864.shape)\n",
    "images_0_8864_class = np.zeros((len(images_0_8864)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of image names\n",
    "images_1_8864 = []\n",
    "for root, dirs, files in os.walk('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8864/1/'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8864/1/'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(2500,)\n",
    "            images_1_8864.append(image_data)\n",
    "            \n",
    "images_1_8864 = np.asarray(images_1_8864)\n",
    "print('There are {} images'.format(len(images_1_8864)))\n",
    "print('The type of the data is', type(images_1_8864))\n",
    "print('The shape of the data is', images_1_8864.shape)\n",
    "images_1_8864_class = np.ones((len(images_1_8864),), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_0_8865 = []\n",
    "for root, dirs, files in os.walk('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8865/0/'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8865/0/'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(2500,)\n",
    "            images_0_8865.append(image_data)\n",
    "            \n",
    "images_0_8865 = np.asarray(images_0_8865)    \n",
    "print('There are {} images'.format(len(images_0_8865)))\n",
    "print('The type of the data is', type(images_0_8865))\n",
    "print('The shape of the data is', images_0_8865.shape)\n",
    "images_0_8865_class = np.zeros((len(images_0_8865)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of image names\n",
    "images_1_8865 = []\n",
    "for root, dirs, files in os.walk('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8865/1/'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('/Users/sophiaperides/Desktop/Thinkful/breast-histopathology-images/8865/1/'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(2500,)\n",
    "            images_1_8865.append(image_data)\n",
    "            \n",
    "images_1_8865 = np.asarray(images_1_8865)\n",
    "print('There are {} images'.format(len(images_1_8865)))\n",
    "print('The type of the data is', type(images_1_8865))\n",
    "print('The shape of the data is', images_1_8865.shape)\n",
    "images_1_8865_class = np.ones((len(images_1_8865),), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining/Cleaning/Normalizing the Data/Images/Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x_train and y_train\n",
    "print(images_0_8863.shape)\n",
    "print(images_1_8863.shape)\n",
    "print(images_0_8863_class.shape)\n",
    "print(images_1_8863_class.shape)\n",
    "\n",
    "\n",
    "x_train = np.append(images_0_8863, images_1_8863, axis=0)\n",
    "y_train = np.append(images_0_8863_class, images_1_8863_class, axis=0)\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "\n",
    "# Create x_test and y_test\n",
    "x_test = np.append(images_0_8865, images_1_8865, axis=0)\n",
    "y_test = np.append(images_0_8865_class, images_1_8865_class, axis=0)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1725 images\n",
      "The type of the data is <class 'numpy.ndarray'>\n",
      "The shape of the data is (1725, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "X_1 = [] # training data labeled 1\n",
    "for root, dirs, files in os.walk('//Users//sophiaperides//Desktop//Thinkful//1'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('/Users/sophiaperides/Desktop/Thinkful/1/'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(type(image_data))\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(1, 50, 50)\n",
    "            X_1.append(image_data)\n",
    "            \n",
    "X_1 = np.asarray(X_1)    \n",
    "print('There are {} images'.format(len(X_1)))\n",
    "print('The type of the data is', type(X_1))\n",
    "print('The shape of the data is', X_1.shape)\n",
    "X_1_target = np.ones((len(X_1),), dtype=int)\n",
    "X_0_target = np.ones((len(X_1),), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7886 images\n",
      "The type of the data is <class 'numpy.ndarray'>\n",
      "The shape of the data is (7886, 50, 50)\n",
      "There are 1725 images once sampled\n"
     ]
    }
   ],
   "source": [
    "X_0 = [] # training data labeled 0\n",
    "for root, dirs, files in os.walk('//Users//sophiaperides//Desktop//Thinkful//0//'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('//Users//sophiaperides//Desktop//Thinkful//0//'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(1, 50, 50)\n",
    "            X_0.append(image_data)\n",
    "            \n",
    "X_0 = np.asarray(X_0)\n",
    "print('There are {} images'.format(len(X_0)))\n",
    "print('The type of the data is', type(X_0))\n",
    "print('The shape of the data is', X_0.shape)\n",
    "X_0 = X_0[:(len(X_1)),:,:]\n",
    "print('There are {} images once sampled'.format(len(X_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 987 images\n",
      "The type of the data is <class 'numpy.ndarray'>\n",
      "The shape of the data is (987, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "X_1_test = [] # testing data labeled 1\n",
    "for root, dirs, files in os.walk('//Users//sophiaperides//Desktop//Thinkful//1_test//'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('//Users//sophiaperides//Desktop//Thinkful//1_test//'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(1, 50, 50)\n",
    "            X_1_test.append(image_data)\n",
    "            \n",
    "X_1_test = np.asarray(X_1_test)    \n",
    "print('There are {} images'.format(len(X_1_test)))\n",
    "print('The type of the data is', type(X_1_test))\n",
    "print('The shape of the data is', X_1_test.shape)\n",
    "X_1_test_target = np.ones((len(X_1_test),), dtype=int)\n",
    "X_0_test_target = np.zeros((len(X_1_test),), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2411 images\n",
      "The type of the data is <class 'numpy.ndarray'>\n",
      "The shape of the data is (2411, 50, 50)\n",
      "There are 987 images once sampled\n"
     ]
    }
   ],
   "source": [
    "X_0_test = [] # testing data labeled 0\n",
    "for root, dirs, files in os.walk('//Users//sophiaperides//Desktop//Thinkful//0_test//'):\n",
    "    for filename in files:\n",
    "        image_data = cv2.imread('//Users//sophiaperides//Desktop//Thinkful//0_test//'+filename, 0)\n",
    "#         print(filename)\n",
    "#         print(image_data.shape)\n",
    "        if image_data.shape==(50,50):\n",
    "#             image_data = image_data.reshape(1, 50, 50)\n",
    "            X_0_test.append(image_data)\n",
    "            \n",
    "X_0_test = np.asarray(X_0_test)    \n",
    "print('There are {} images'.format(len(X_0_test)))\n",
    "print('The type of the data is', type(X_0_test))\n",
    "print('The shape of the data is', X_0_test.shape)\n",
    "X_0_test = X_0_test[:(len(X_1_test)),:,:]\n",
    "print('There are {} images once sampled'.format(len(X_0_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining/Cleaning/Normalizing the Data/Images/Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1725, 50, 50)\n",
      "(1725, 50, 50)\n",
      "(1725,)\n",
      "(1725,)\n",
      "x_train shape: (3450, 50, 50)\n",
      "y_train shape: (3450,)\n",
      "x_test shape: (1974, 50, 50)\n",
      "y_test shape:  (1974,)\n"
     ]
    }
   ],
   "source": [
    "# Create x_train and y_train\n",
    "print(X_0.shape)\n",
    "print(X_1.shape)\n",
    "print(X_0_target.shape)\n",
    "print(X_1_target.shape)\n",
    "\n",
    "\n",
    "x_train = np.append(X_1, X_0, axis=0)\n",
    "y_train = np.append(X_1_target, X_0_target, axis=0)\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "\n",
    "# Create x_test and y_test\n",
    "x_test = np.append(X_1_test, X_0_test, axis=0)\n",
    "y_test = np.append(X_1_test_target, X_0_test_target, axis=0)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3450, 50, 50)\n",
      "x_test shape: (1974, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, create 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_train, 2)\n",
    "y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per Yunus: first of all keep your images as (1,50,50) not as (1,2500).\n",
    "img_rows = 50\n",
    "img_cols = 50\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with TanH Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3450 samples, validate on 1974 samples\n",
      "Epoch 1/4\n",
      "3328/3450 [===========================>..] - ETA: 0s - loss: 16.0596 - accuracy: 0.9820"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-58659205a4b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "num_classes = 2\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='tanh',\n",
    "                 input_shape=input_shape,  kernel_regularizer=regularizers.l2(0.5)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (4, 4), activation='tanh',  kernel_regularizer=regularizers.l2(0.5)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='tanh',  kernel_regularizer=regularizers.l2(0.5)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax',  kernel_regularizer=regularizers.l2(0.5)))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=4,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Relu Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3450 samples, validate on 1974 samples\n",
      "Epoch 1/10\n",
      "3450/3450 [==============================] - 18s 5ms/step - loss: 183.0240 - accuracy: 0.9925 - val_loss: 75.0515 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "3450/3450 [==============================] - 16s 5ms/step - loss: 26.3969 - accuracy: 1.0000 - val_loss: 9.6459 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "3450/3450 [==============================] - 12s 4ms/step - loss: 6.5854 - accuracy: 1.0000 - val_loss: 10.6332 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "3450/3450 [==============================] - 14s 4ms/step - loss: 6.7594 - accuracy: 1.0000 - val_loss: 11.1406 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "3450/3450 [==============================] - 12s 3ms/step - loss: 6.9235 - accuracy: 1.0000 - val_loss: 11.5760 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "3450/3450 [==============================] - 14s 4ms/step - loss: 7.0874 - accuracy: 1.0000 - val_loss: 11.9244 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "3450/3450 [==============================] - 12s 4ms/step - loss: 7.2435 - accuracy: 1.0000 - val_loss: 12.2924 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "3450/3450 [==============================] - 15s 4ms/step - loss: 7.4003 - accuracy: 1.0000 - val_loss: 12.6015 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "3450/3450 [==============================] - 14s 4ms/step - loss: 7.5498 - accuracy: 1.0000 - val_loss: 12.9493 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "3450/3450 [==============================] - 12s 4ms/step - loss: 7.7004 - accuracy: 1.0000 - val_loss: 13.2245 - val_accuracy: 0.5000\n",
      "Test loss: 13.224491134844532\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "num_classes = 2\n",
    "model_cnn_relu = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model_cnn_relu.add(Conv2D(32, kernel_size=(4, 4),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model_cnn_relu.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn_relu.add(Conv2D(32, (4, 4), activation='relu', kernel_regularizer=regularizers.l1(0.5)))\n",
    "model_cnn_relu.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn_relu.add(Dropout(0.5))\n",
    "model_cnn_relu.add(Flatten())\n",
    "# model_cnn_relu.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.75)))\n",
    "# model_cnn_relu.add(Dropout(0.5))\n",
    "model_cnn_relu.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_cnn_relu.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_cnn_relu.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model_cnn_relu.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 7 with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   rotation_range=45,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   validation_split = .2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  validation_split = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7818 images belonging to 2 classes.\n",
      "Found 679 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('/Users/sophiaperides/Desktop/Thinkful/breast_tissue_train',\n",
    "                                       target_size=(50, 50),\n",
    "                                                batch_size=32,\n",
    "                                     class_mode='categorical',\n",
    "                                            subset='training',\n",
    "                                            color_mode='grayscale')\n",
    "\n",
    "validation_set = test_datagen.flow_from_directory('/Users/sophiaperides/Desktop/Thinkful/breast_tissue_test',\n",
    "                                        target_size=(50, 50),\n",
    "                                                 batch_size=32,\n",
    "                                      class_mode='categorical',\n",
    "                                               shuffle = False,\n",
    "                                           subset='validation',\n",
    "                                             color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 [==============================] - 86s 349ms/step - loss: 0.4387 - accuracy: 0.8223 - val_loss: 1.3782 - val_accuracy: 0.7099\n",
      "Epoch 2/10\n",
      "245/245 [==============================] - 82s 335ms/step - loss: 0.3814 - accuracy: 0.8466 - val_loss: 1.7135 - val_accuracy: 0.7099\n",
      "Epoch 3/10\n",
      "245/245 [==============================] - 90s 365ms/step - loss: 0.3604 - accuracy: 0.8539 - val_loss: 0.6869 - val_accuracy: 0.8306\n",
      "Epoch 4/10\n",
      "245/245 [==============================] - 94s 385ms/step - loss: 0.3617 - accuracy: 0.8588 - val_loss: 1.3598 - val_accuracy: 0.7099\n",
      "Epoch 5/10\n",
      "245/245 [==============================] - 95s 387ms/step - loss: 0.3546 - accuracy: 0.8583 - val_loss: 1.4419 - val_accuracy: 0.7069\n",
      "Epoch 6/10\n",
      "245/245 [==============================] - 95s 388ms/step - loss: 0.3499 - accuracy: 0.8597 - val_loss: 0.8245 - val_accuracy: 0.8144\n",
      "Epoch 7/10\n",
      "245/245 [==============================] - 94s 384ms/step - loss: 0.3464 - accuracy: 0.8602 - val_loss: 0.8848 - val_accuracy: 0.7747\n",
      "Epoch 8/10\n",
      "245/245 [==============================] - 124s 504ms/step - loss: 0.3458 - accuracy: 0.8613 - val_loss: 1.2450 - val_accuracy: 0.7084\n",
      "Epoch 9/10\n",
      "245/245 [==============================] - 130s 530ms/step - loss: 0.3430 - accuracy: 0.8606 - val_loss: 0.7919 - val_accuracy: 0.8174\n",
      "Epoch 10/10\n",
      "245/245 [==============================] - 99s 402ms/step - loss: 0.3462 - accuracy: 0.8605 - val_loss: 1.0863 - val_accuracy: 0.7113\n",
      "Test loss: 0.5953276348814535\n",
      "Test accuracy: 0.7264437675476074\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "num_classes = 2\n",
    "model_7 = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model_7.add(Conv2D(32, kernel_size=(4, 4),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model_7.add(Conv2D(64, (4, 4), activation='relu', ))\n",
    "model_7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_7.add(Dropout(0.25))\n",
    "model_7.add(Flatten())\n",
    "model_7.add(Dense(128, activation='relu'))\n",
    "model_7.add(Dropout(0.5))\n",
    "model_7.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_7.compile(loss=ker`as.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_7.fit_generator(training_set,\n",
    "                    steps_per_epoch=len(training_set),\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_set,\n",
    "                    validation_steps=len(validation_set))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model_7.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- keep same activation \n",
    "- google collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 [==============================] - 102s 417ms/step - loss: 0.5044 - accuracy: 0.8172 - val_loss: 1.0771 - val_accuracy: 0.7099\n",
      "Epoch 2/10\n",
      "245/245 [==============================] - 88s 359ms/step - loss: 0.3981 - accuracy: 0.8381 - val_loss: 1.1201 - val_accuracy: 0.7113\n",
      "Epoch 3/10\n",
      "245/245 [==============================] - 86s 353ms/step - loss: 0.3762 - accuracy: 0.8439 - val_loss: 0.5793 - val_accuracy: 0.7761\n",
      "Epoch 4/10\n",
      "245/245 [==============================] - 90s 369ms/step - loss: 0.3797 - accuracy: 0.8427 - val_loss: 0.7065 - val_accuracy: 0.7202\n",
      "Epoch 5/10\n",
      "245/245 [==============================] - 97s 396ms/step - loss: 0.3574 - accuracy: 0.8526 - val_loss: 0.7455 - val_accuracy: 0.7246\n",
      "Epoch 6/10\n",
      "245/245 [==============================] - 112s 456ms/step - loss: 0.3616 - accuracy: 0.8523 - val_loss: 0.6047 - val_accuracy: 0.7629\n",
      "Epoch 7/10\n",
      "245/245 [==============================] - 102s 417ms/step - loss: 0.3626 - accuracy: 0.8506 - val_loss: 0.4278 - val_accuracy: 0.8292\n",
      "Epoch 8/10\n",
      "245/245 [==============================] - 105s 430ms/step - loss: 0.3586 - accuracy: 0.8518 - val_loss: 0.7286 - val_accuracy: 0.7246\n",
      "Epoch 9/10\n",
      "245/245 [==============================] - 103s 422ms/step - loss: 0.3591 - accuracy: 0.8514 - val_loss: 0.6629 - val_accuracy: 0.7526\n",
      "Epoch 10/10\n",
      "245/245 [==============================] - 112s 458ms/step - loss: 0.3612 - accuracy: 0.8520 - val_loss: 0.7252 - val_accuracy: 0.7496\n",
      "Test loss: 0.5077049956556028\n",
      "Test accuracy: 0.7674772143363953\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "num_classes = 2\n",
    "model_8 = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model_8.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='tanh',\n",
    "                 input_shape=input_shape))\n",
    "model_8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_8.add(Conv2D(64, (4, 4), activation='tanh', ))\n",
    "model_8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_8.add(Dropout(0.25))\n",
    "model_8.add(Flatten())\n",
    "model_8.add(Dense(128, activation='tanh'))\n",
    "model_8.add(Dropout(0.5))\n",
    "model_8.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model_8.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_8.fit_generator(training_set,\n",
    "                    steps_per_epoch=len(training_set),\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_set,\n",
    "                    validation_steps=len(validation_set))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model_8.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[908,  79],\n",
       "       [380, 607]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_8.predict(x_test)\n",
    "y_pred = (predictions > 0.5)\n",
    "matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|| Class 1 Predicted | Class 2 Predicted   |\n",
    "|------|------|------|\n",
    "Actual Class 1| 908 | 79 |\n",
    "Actual Class 2 | 380 | 607 |\n",
    "\n",
    "|| Class 1 Predicted | Class 2 Predicted   |\n",
    "|------|------|------|\n",
    "Actual Class 1| 70.5% | 11.5% |\n",
    "Actual Class 2 | 29.5% | 88.5% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 [==============================] - 135s 551ms/step - loss: 7.7453 - accuracy: 0.8200 - val_loss: 1.6359 - val_accuracy: 0.7099\n",
      "Epoch 2/10\n",
      "245/245 [==============================] - 133s 541ms/step - loss: 0.5066 - accuracy: 0.8232 - val_loss: 1.7381 - val_accuracy: 0.7099\n",
      "Epoch 3/10\n",
      "245/245 [==============================] - 134s 545ms/step - loss: 0.4933 - accuracy: 0.8232 - val_loss: 1.8709 - val_accuracy: 0.7099\n",
      "Epoch 4/10\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.4820 - accuracy: 0.8232 - val_loss: 1.3573 - val_accuracy: 0.7099\n",
      "Epoch 5/10\n",
      "245/245 [==============================] - 137s 558ms/step - loss: 0.4698 - accuracy: 0.8232 - val_loss: 1.5142 - val_accuracy: 0.7099\n",
      "Epoch 6/10\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.4618 - accuracy: 0.8232 - val_loss: 1.5105 - val_accuracy: 0.7099\n",
      "Epoch 7/10\n",
      "245/245 [==============================] - 133s 543ms/step - loss: 0.4576 - accuracy: 0.8232 - val_loss: 1.5464 - val_accuracy: 0.7099\n",
      "Epoch 8/10\n",
      "245/245 [==============================] - 133s 543ms/step - loss: 0.4547 - accuracy: 0.8232 - val_loss: 1.7961 - val_accuracy: 0.7099\n",
      "Epoch 9/10\n",
      "245/245 [==============================] - 135s 549ms/step - loss: 0.4529 - accuracy: 0.8232 - val_loss: 1.6427 - val_accuracy: 0.7099\n",
      "Epoch 10/10\n",
      "245/245 [==============================] - 184s 752ms/step - loss: 0.4494 - accuracy: 0.8232 - val_loss: 1.3515 - val_accuracy: 0.7099\n",
      "Test loss: 0.7974979342117136\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "num_classes = 2\n",
    "model_8 = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model_8.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='tanh',\n",
    "                 input_shape=input_shape))\n",
    "model_8.add(Conv2D(64, (4, 4), activation='tah', padding='same'))\n",
    "model_8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_8.add(Dropout(0.35))\n",
    "model_8.add(Flatten())\n",
    "model_8.add(Dense(128, activation='tanh', kernel_regularizer=regularizers.l2(1)))\n",
    "model_8.add(Dropout(0.5))\n",
    "model_8.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_8.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_8.fit_generator(training_set,\n",
    "                    steps_per_epoch=len(training_set),\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_set,\n",
    "                    validation_steps=len(validation_set))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model_8.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 [==============================] - 80s 329ms/step - loss: 0.4589 - accuracy: 0.8205 - val_loss: 1.0422 - val_accuracy: 0.7099\n",
      "Epoch 2/10\n",
      "245/245 [==============================] - 99s 402ms/step - loss: 0.3968 - accuracy: 0.8262 - val_loss: 0.9220 - val_accuracy: 0.7894\n",
      "Epoch 3/10\n",
      "245/245 [==============================] - 72s 293ms/step - loss: 0.3695 - accuracy: 0.8510 - val_loss: 1.0121 - val_accuracy: 0.8012\n",
      "Epoch 4/10\n",
      "245/245 [==============================] - 71s 290ms/step - loss: 0.3606 - accuracy: 0.8529 - val_loss: 0.7464 - val_accuracy: 0.8468\n",
      "Epoch 5/10\n",
      "245/245 [==============================] - 72s 294ms/step - loss: 0.3602 - accuracy: 0.8507 - val_loss: 1.4011 - val_accuracy: 0.7629\n",
      "Epoch 6/10\n",
      "245/245 [==============================] - 71s 291ms/step - loss: 0.3560 - accuracy: 0.8535 - val_loss: 1.0661 - val_accuracy: 0.7747\n",
      "Epoch 7/10\n",
      "245/245 [==============================] - 72s 293ms/step - loss: 0.3537 - accuracy: 0.8558 - val_loss: 1.1106 - val_accuracy: 0.7923\n",
      "Epoch 8/10\n",
      "245/245 [==============================] - 72s 295ms/step - loss: 0.3471 - accuracy: 0.8608 - val_loss: 1.5401 - val_accuracy: 0.7084\n",
      "Epoch 9/10\n",
      "245/245 [==============================] - 73s 299ms/step - loss: 0.3417 - accuracy: 0.8667 - val_loss: 1.1543 - val_accuracy: 0.7791\n",
      "Epoch 10/10\n",
      "245/245 [==============================] - 72s 292ms/step - loss: 0.3375 - accuracy: 0.8643 - val_loss: 0.7305 - val_accuracy: 0.8527\n",
      "Test loss: 0.5438061677817757\n",
      "Test accuracy: 0.7502533197402954\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape, activation = 'relu'))\n",
    "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Dropout(0.5)) # antes era 0.25\n",
    "\n",
    " \n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(64, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Dropout(0.25)) # antes era 0.25\n",
    "\n",
    " \n",
    "\n",
    "# Adding a third convolutional layer\n",
    "classifier.add(Conv2D(64, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Dropout(0.5)) # antes era 0.25\n",
    "\n",
    " \n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    " \n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 512, activation = 'relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "classifier.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                    steps_per_epoch=len(training_set),\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_set,\n",
    "                    validation_steps=len(validation_set))\n",
    "\n",
    "# Evaluation.\n",
    "scores = classifier.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.append(images_0_8863, images_1_8863, axis=0)\n",
    "target = np.append(images_0_8863_class, images_1_8863_class, axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target,\n",
    "                test_size = .3, random_state = 465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 [==============================] - 64s 262ms/step - loss: 0.4469 - accuracy: 0.8226 - val_loss: 0.7352 - val_accuracy: 0.7275\n",
      "Epoch 2/10\n",
      "245/245 [==============================] - 32s 130ms/step - loss: 0.3674 - accuracy: 0.8456 - val_loss: 0.5828 - val_accuracy: 0.7938\n",
      "Epoch 3/10\n",
      "245/245 [==============================] - 32s 131ms/step - loss: 0.3610 - accuracy: 0.8483 - val_loss: 0.9606 - val_accuracy: 0.7246\n",
      "Epoch 4/10\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 0.3557 - accuracy: 0.8515 - val_loss: 0.7928 - val_accuracy: 0.7909\n",
      "Epoch 5/10\n",
      "245/245 [==============================] - 37s 150ms/step - loss: 0.3504 - accuracy: 0.8557 - val_loss: 0.8205 - val_accuracy: 0.7496\n",
      "Epoch 6/10\n",
      "245/245 [==============================] - 60s 244ms/step - loss: 0.3529 - accuracy: 0.8551 - val_loss: 0.8317 - val_accuracy: 0.7585\n",
      "Epoch 7/10\n",
      "245/245 [==============================] - 34s 138ms/step - loss: 0.3482 - accuracy: 0.8555 - val_loss: 1.2666 - val_accuracy: 0.7113\n",
      "Epoch 8/10\n",
      "245/245 [==============================] - 33s 133ms/step - loss: 0.3473 - accuracy: 0.8557 - val_loss: 1.0876 - val_accuracy: 0.7202\n",
      "Epoch 9/10\n",
      "245/245 [==============================] - 33s 134ms/step - loss: 0.3477 - accuracy: 0.8564 - val_loss: 1.2163 - val_accuracy: 0.7128\n",
      "Epoch 10/10\n",
      "245/245 [==============================] - 35s 144ms/step - loss: 0.3474 - accuracy: 0.8543 - val_loss: 1.3704 - val_accuracy: 0.7128\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_90_input to have 4 dimensions, but got array with shape (293, 50, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-ecc85fd30f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Evaluation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_90_input to have 4 dimensions, but got array with shape (293, 50, 50)"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "num_classes = 2\n",
    "model_8 = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model_8.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='tanh',\n",
    "                 input_shape=input_shape))\n",
    "model_8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_8.add(Conv2D(64, (4, 4), activation='tanh', ))\n",
    "model_8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_8.add(Dropout(0.25))\n",
    "model_8.add(Flatten())\n",
    "model_8.add(Dense(128, activation='tanh'))\n",
    "model_8.add(Dropout(0.5))\n",
    "model_8.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model_8.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_8.fit_generator(training_set,\n",
    "                    steps_per_epoch=len(training_set),\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_set,\n",
    "                    validation_steps=len(validation_set))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model_8.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
