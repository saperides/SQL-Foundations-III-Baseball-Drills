{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boost guided example\n",
    "\n",
    "Having walked through gradient boost by hand, now let's try it with SKlearn.  We'll still use the European Social Survey Data, but now with a categorical outcome: Whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()\n",
    "\n",
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're now working with a binary outcome, we've switched to a classifier.  Now our loss function can't be the residuals.  Our options are \"deviance\", or \"exponential\".  Deviance is used for logistic regression, and we'll try that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04650845608292417\n",
      "Percent Type II errors: 0.17607746863066012\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06257668711656442\n",
      "Percent Type II errors: 0.18527607361963191\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike decision trees, gradient boost solutions are not terribly easy to interpret on the surface. But they aren't quite a black box. We can get a measure of how important various features are by counting how many times a feature is used over the course of many decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that age and happiness are the most important features in predicting whether or not someone lives with a partner.\n",
    "\n",
    "### DRILL: Improve this gradient boost model\n",
    "\n",
    "While this model is already doing alright, we've seen from the Type I and Type II error rates that there is definitely room for improvement.  Your task is to see how low you can get the error rates to go in the test set, based on your model in the training set.  Strategies you might use include:\n",
    "\n",
    "* Creating new features\n",
    "* Applying more overfitting-prevention strategies like subsampling\n",
    "* More iterations\n",
    "* Trying a different loss function\n",
    "* Changing the structure of the weak learner: Allowing more leaves in the tree, or other modifications\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cntry</th>\n",
       "      <th>idno</th>\n",
       "      <th>year</th>\n",
       "      <th>tvtot</th>\n",
       "      <th>ppltrst</th>\n",
       "      <th>pplfair</th>\n",
       "      <th>pplhlp</th>\n",
       "      <th>happy</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>sclact</th>\n",
       "      <th>gndr</th>\n",
       "      <th>agea</th>\n",
       "      <th>partner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CH</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CH</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CH</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CH</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CH</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cntry  idno  year  tvtot  ppltrst  pplfair  pplhlp  happy  sclmeet  sclact  \\\n",
       "0    CH   5.0     6    3.0      3.0     10.0     5.0    8.0      5.0     4.0   \n",
       "1    CH  25.0     6    6.0      5.0      7.0     5.0    9.0      3.0     2.0   \n",
       "2    CH  26.0     6    1.0      8.0      8.0     8.0    7.0      6.0     3.0   \n",
       "3    CH  28.0     6    4.0      6.0      6.0     7.0   10.0      6.0     2.0   \n",
       "4    CH  29.0     6    5.0      6.0      7.0     5.0    8.0      7.0     2.0   \n",
       "\n",
       "   gndr  agea  partner  \n",
       "0   2.0  60.0      1.0  \n",
       "1   2.0  59.0      1.0  \n",
       "2   1.0  24.0      2.0  \n",
       "3   2.0  64.0      1.0  \n",
       "4   2.0  55.0      1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While updating the tree depth to five levels decreases the both types of training errors substantially, it\n",
      "increases Type 1 errors in the test set substantially.\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.0061374795417348605\n",
      "Percent Type II errors: 0.06860338243316967\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.10429447852760736\n",
      "Percent Type II errors: 0.17300613496932515\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 5-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 5,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('While updating the tree depth to five levels decreases the both types of training errors substantially, it')\n",
    "print('increases Type 1 errors in the test set.\\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the iterations to 1,000 seems to have a slightly negative effect on the test set, so we'll use the\n",
      "original parameters of 500 trees and a depth of 2 moving forward. \n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.044189852700491\n",
      "Percent Type II errors: 0.1692580469176214\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.07116564417177915\n",
      "Percent Type II errors: 0.18036809815950922\n"
     ]
    }
   ],
   "source": [
    "# We'll make 1,000 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Updating the iterations to 1,000 seems to have a slightly negative effect on the test set, so we\\'ll use the')\n",
    "print('original parameters of 500 trees and a depth of 2 moving forward. \\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the learning rate to 0.9 slightly increases both types of errors in the test set.\n",
      "Changing the learning rate to 0.01 decreases Type I errors in both the training and test sets, but\n",
      "increases Type II errors. Thus I will leave the learning rate at the default of 0.01. \n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.036552100381887616\n",
      "Percent Type II errors: 0.1395253682487725\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.08834355828220859\n",
      "Percent Type II errors: 0.18159509202453988\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, set our loss function, and update the learning rate to 0.5\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance',\n",
    "         'learning_rate': 0.9}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Updating the learning rate to 0.9 slightly increases both types of errors in the test set.')\n",
    "print('Changing the learning rate to 0.01 decreases Type I errors in both the training and test sets, but')\n",
    "print('increases Type II errors. Thus I will leave the learning rate at the default of 0.01. \\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the subsample to 0.25 (testing 75% of the data) surprisingly doesn't improve the percentage of errors.\n",
      "I don't seem to be making much headway here...\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04800872885979269\n",
      "Percent Type II errors: 0.171303873431533\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.0736196319018405\n",
      "Percent Type II errors: 0.17791411042944785\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees,  and subsample to 0.25\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance',\n",
    "         'subsample': 0.25}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Updating the subsample to 0.25 (testing 75% of the data) surprisingly doesn\\'t improve the percentage of errors.')\n",
    "print('I don\\'t seem to be making much headway here...\\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the number of iterations to 1,000, the max depth to 4, the subsample to 0.2, and the learning rate\n",
      "to 0.25 just to see what happens. All errors increased substantially, with the exception of Type II errors\n",
      "in the Test set. \n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.12629569012547737\n",
      "Percent Type II errors: 0.14961811238406983\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.17423312883435582\n",
      "Percent Type II errors: 0.14969325153374233\n"
     ]
    }
   ],
   "source": [
    "# We'll make 1000 iterations, use 4-deep trees, update the sample space to 0.2, and update the learning rate to 0.25\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 4,\n",
    "          'loss': 'deviance',\n",
    "         'subsample': 0.2,\n",
    "         'learning_rate': 0.25}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Changing the number of iterations to 1,000, the max depth to 4, the subsample to 0.2, and the learning rate')\n",
    "print('to 0.25 just to see what happens. All errors increased substantially, with the exception of Type II errors')\n",
    "print('in the Test set. \\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the loss type to exponential minimally decreases the Type I errors and increases the Type II errors\n",
      "in the Test set. \n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04841789416257501\n",
      "Percent Type II errors: 0.18262411347517732\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05889570552147239\n",
      "Percent Type II errors: 0.1901840490797546\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, update the sample space to 0.2, and update the learning rate to 0.25\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'exponential'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Changing the loss type to exponential minimally decreases the Type I errors and increases the Type II errors')\n",
    "print('in the Test set. \\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set accuracy:\n",
    "Percent Type I errors: 0.04650845608292417\n",
    "Percent Type II errors: 0.17607746863066012\n",
    "\n",
    "Test set accuracy:\n",
    "Percent Type I errors: 0.06257668711656442\n",
    "Percent Type II errors: 0.18527607361963191\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cntry', 'idno', 'year', 'tvtot', 'ppltrst', 'pplfair', 'pplhlp',\n",
       "       'happy', 'sclmeet', 'sclact', 'gndr', 'agea', 'partner'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idno</th>\n",
       "      <th>year</th>\n",
       "      <th>tvtot</th>\n",
       "      <th>ppltrst</th>\n",
       "      <th>pplfair</th>\n",
       "      <th>pplhlp</th>\n",
       "      <th>happy</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>sclact</th>\n",
       "      <th>gndr</th>\n",
       "      <th>agea</th>\n",
       "      <th>partner</th>\n",
       "      <th>ppl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idno</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>-0.007523</td>\n",
       "      <td>0.007397</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>-0.003065</td>\n",
       "      <td>-0.021376</td>\n",
       "      <td>0.004323</td>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.030051</td>\n",
       "      <td>-0.014931</td>\n",
       "      <td>-0.002619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.002102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031596</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>-0.032901</td>\n",
       "      <td>-0.013378</td>\n",
       "      <td>-0.028502</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>0.018689</td>\n",
       "      <td>-0.000631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tvtot</th>\n",
       "      <td>0.003963</td>\n",
       "      <td>-0.031596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.142422</td>\n",
       "      <td>-0.119277</td>\n",
       "      <td>-0.069080</td>\n",
       "      <td>-0.118598</td>\n",
       "      <td>-0.078864</td>\n",
       "      <td>-0.092375</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.257674</td>\n",
       "      <td>-0.028816</td>\n",
       "      <td>-0.143054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppltrst</th>\n",
       "      <td>-0.007523</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>-0.142422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.459250</td>\n",
       "      <td>0.231533</td>\n",
       "      <td>0.122555</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>-0.029921</td>\n",
       "      <td>-0.029412</td>\n",
       "      <td>-0.034371</td>\n",
       "      <td>0.885843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplfair</th>\n",
       "      <td>0.007397</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>-0.119277</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.480931</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.128808</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>0.836297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplhlp</th>\n",
       "      <td>0.004484</td>\n",
       "      <td>-0.032901</td>\n",
       "      <td>-0.069080</td>\n",
       "      <td>0.459250</td>\n",
       "      <td>0.480931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.215323</td>\n",
       "      <td>0.080489</td>\n",
       "      <td>0.092673</td>\n",
       "      <td>0.042046</td>\n",
       "      <td>0.040351</td>\n",
       "      <td>-0.022069</td>\n",
       "      <td>0.498306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>-0.003065</td>\n",
       "      <td>-0.013378</td>\n",
       "      <td>-0.118598</td>\n",
       "      <td>0.231533</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.215323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182944</td>\n",
       "      <td>0.192030</td>\n",
       "      <td>-0.022413</td>\n",
       "      <td>-0.042970</td>\n",
       "      <td>-0.145061</td>\n",
       "      <td>0.260113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sclmeet</th>\n",
       "      <td>-0.021376</td>\n",
       "      <td>-0.028502</td>\n",
       "      <td>-0.078864</td>\n",
       "      <td>0.122555</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.080489</td>\n",
       "      <td>0.182944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283319</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>-0.194443</td>\n",
       "      <td>0.162970</td>\n",
       "      <td>0.125017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sclact</th>\n",
       "      <td>0.004323</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>-0.092375</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>0.128808</td>\n",
       "      <td>0.092673</td>\n",
       "      <td>0.192030</td>\n",
       "      <td>0.283319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>-0.059442</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>0.145811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gndr</th>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>-0.029921</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>0.042046</td>\n",
       "      <td>-0.022413</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>-0.002641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agea</th>\n",
       "      <td>0.030051</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>0.257674</td>\n",
       "      <td>-0.029412</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.040351</td>\n",
       "      <td>-0.042970</td>\n",
       "      <td>-0.194443</td>\n",
       "      <td>-0.059442</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.256670</td>\n",
       "      <td>0.002273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partner</th>\n",
       "      <td>-0.014931</td>\n",
       "      <td>0.018689</td>\n",
       "      <td>-0.028816</td>\n",
       "      <td>-0.034371</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>-0.022069</td>\n",
       "      <td>-0.145061</td>\n",
       "      <td>0.162970</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>-0.256670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl</th>\n",
       "      <td>-0.002619</td>\n",
       "      <td>-0.000631</td>\n",
       "      <td>-0.143054</td>\n",
       "      <td>0.885843</td>\n",
       "      <td>0.836297</td>\n",
       "      <td>0.498306</td>\n",
       "      <td>0.260113</td>\n",
       "      <td>0.125017</td>\n",
       "      <td>0.145811</td>\n",
       "      <td>-0.002641</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>-0.045146</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             idno      year     tvtot   ppltrst   pplfair    pplhlp     happy  \\\n",
       "idno     1.000000  0.002102  0.003963 -0.007523  0.007397  0.004484 -0.003065   \n",
       "year     0.002102  1.000000 -0.031596 -0.001600  0.004283 -0.032901 -0.013378   \n",
       "tvtot    0.003963 -0.031596  1.000000 -0.142422 -0.119277 -0.069080 -0.118598   \n",
       "ppltrst -0.007523 -0.001600 -0.142422  1.000000  0.597506  0.459250  0.231533   \n",
       "pplfair  0.007397  0.004283 -0.119277  0.597506  1.000000  0.480931  0.247755   \n",
       "pplhlp   0.004484 -0.032901 -0.069080  0.459250  0.480931  1.000000  0.215323   \n",
       "happy   -0.003065 -0.013378 -0.118598  0.231533  0.247755  0.215323  1.000000   \n",
       "sclmeet -0.021376 -0.028502 -0.078864  0.122555  0.096501  0.080489  0.182944   \n",
       "sclact   0.004323  0.012150 -0.092375  0.137491  0.128808  0.092673  0.192030   \n",
       "gndr     0.010851  0.001355  0.017922 -0.029921  0.022251  0.042046 -0.022413   \n",
       "agea     0.030051 -0.003520  0.257674 -0.029412  0.014724  0.040351 -0.042970   \n",
       "partner -0.014931  0.018689 -0.028816 -0.034371 -0.034054 -0.022069 -0.145061   \n",
       "ppl     -0.002619 -0.000631 -0.143054  0.885843  0.836297  0.498306  0.260113   \n",
       "\n",
       "          sclmeet    sclact      gndr      agea   partner       ppl  \n",
       "idno    -0.021376  0.004323  0.010851  0.030051 -0.014931 -0.002619  \n",
       "year    -0.028502  0.012150  0.001355 -0.003520  0.018689 -0.000631  \n",
       "tvtot   -0.078864 -0.092375  0.017922  0.257674 -0.028816 -0.143054  \n",
       "ppltrst  0.122555  0.137491 -0.029921 -0.029412 -0.034371  0.885843  \n",
       "pplfair  0.096501  0.128808  0.022251  0.014724 -0.034054  0.836297  \n",
       "pplhlp   0.080489  0.092673  0.042046  0.040351 -0.022069  0.498306  \n",
       "happy    0.182944  0.192030 -0.022413 -0.042970 -0.145061  0.260113  \n",
       "sclmeet  1.000000  0.283319  0.009533 -0.194443  0.162970  0.125017  \n",
       "sclact   0.283319  1.000000 -0.031648 -0.059442  0.011405  0.145811  \n",
       "gndr     0.009533 -0.031648  1.000000  0.020598  0.033984 -0.002641  \n",
       "agea    -0.194443 -0.059442  0.020598  1.000000 -0.256670  0.002273  \n",
       "partner  0.162970  0.011405  0.033984 -0.256670  1.000000 -0.045146  \n",
       "ppl      0.125017  0.145811 -0.002641  0.002273 -0.045146  1.000000  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ppl'] = df['pplfair'] * df['ppltrst'] * df['pplhlp']\n",
    "\n",
    "X = df.loc[:, ['happy',  'agea', 'ppl']]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the number of iterations to 1,000, the max depth to 4, the subsample to 0.2, and the learning rate\n",
      "to 0.25 just to see what happens. Results are varied, but there's a big enough jump in Type I errors\n",
      " in the Test set to discourage me from using this model.\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04841789416257501\n",
      "Percent Type II errors: 0.1778505182760502\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.0638036809815951\n",
      "Percent Type II errors: 0.18773006134969325\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and changing the features\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'exponential'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Changing the number of iterations to 1,000, the max depth to 4, the subsample to 0.2, and the learning rate')\n",
    "print('to 0.25 just to see what happens. Results are varied, but there\\'s a big enough jump in Type I errors')\n",
    "print(' in the Test set to discourage me from using this model.\\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idno</th>\n",
       "      <th>year</th>\n",
       "      <th>tvtot</th>\n",
       "      <th>ppltrst</th>\n",
       "      <th>pplfair</th>\n",
       "      <th>pplhlp</th>\n",
       "      <th>happy</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>sclact</th>\n",
       "      <th>gndr</th>\n",
       "      <th>agea</th>\n",
       "      <th>partner</th>\n",
       "      <th>CH</th>\n",
       "      <th>CZ</th>\n",
       "      <th>DE</th>\n",
       "      <th>ES</th>\n",
       "      <th>NO</th>\n",
       "      <th>SE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idno</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>-0.007523</td>\n",
       "      <td>0.007397</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>-0.003065</td>\n",
       "      <td>-0.021376</td>\n",
       "      <td>0.004323</td>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.030051</td>\n",
       "      <td>-0.014931</td>\n",
       "      <td>-0.028832</td>\n",
       "      <td>-0.025586</td>\n",
       "      <td>0.999965</td>\n",
       "      <td>-0.038364</td>\n",
       "      <td>-0.018853</td>\n",
       "      <td>-0.031512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.002102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031596</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>-0.032901</td>\n",
       "      <td>-0.013378</td>\n",
       "      <td>-0.028502</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>0.018689</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>-0.006935</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.002085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tvtot</th>\n",
       "      <td>0.003963</td>\n",
       "      <td>-0.031596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.142422</td>\n",
       "      <td>-0.119277</td>\n",
       "      <td>-0.069080</td>\n",
       "      <td>-0.118598</td>\n",
       "      <td>-0.078864</td>\n",
       "      <td>-0.092375</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.257674</td>\n",
       "      <td>-0.028816</td>\n",
       "      <td>-0.173825</td>\n",
       "      <td>0.218606</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.035985</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>-0.050075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppltrst</th>\n",
       "      <td>-0.007523</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>-0.142422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.459250</td>\n",
       "      <td>0.231533</td>\n",
       "      <td>0.122555</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>-0.029921</td>\n",
       "      <td>-0.029412</td>\n",
       "      <td>-0.034371</td>\n",
       "      <td>0.031936</td>\n",
       "      <td>-0.225021</td>\n",
       "      <td>-0.009180</td>\n",
       "      <td>-0.152500</td>\n",
       "      <td>0.217530</td>\n",
       "      <td>0.132684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplfair</th>\n",
       "      <td>0.007397</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>-0.119277</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.480931</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.128808</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>0.058250</td>\n",
       "      <td>-0.215494</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>-0.188744</td>\n",
       "      <td>0.191488</td>\n",
       "      <td>0.161542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplhlp</th>\n",
       "      <td>0.004484</td>\n",
       "      <td>-0.032901</td>\n",
       "      <td>-0.069080</td>\n",
       "      <td>0.459250</td>\n",
       "      <td>0.480931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.215323</td>\n",
       "      <td>0.080489</td>\n",
       "      <td>0.092673</td>\n",
       "      <td>0.042046</td>\n",
       "      <td>0.040351</td>\n",
       "      <td>-0.022069</td>\n",
       "      <td>0.079003</td>\n",
       "      <td>-0.166372</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>-0.223920</td>\n",
       "      <td>0.153183</td>\n",
       "      <td>0.173921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>-0.003065</td>\n",
       "      <td>-0.013378</td>\n",
       "      <td>-0.118598</td>\n",
       "      <td>0.231533</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.215323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182944</td>\n",
       "      <td>0.192030</td>\n",
       "      <td>-0.022413</td>\n",
       "      <td>-0.042970</td>\n",
       "      <td>-0.145061</td>\n",
       "      <td>0.110593</td>\n",
       "      <td>-0.206445</td>\n",
       "      <td>-0.003884</td>\n",
       "      <td>-0.074060</td>\n",
       "      <td>0.100777</td>\n",
       "      <td>0.063745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sclmeet</th>\n",
       "      <td>-0.021376</td>\n",
       "      <td>-0.028502</td>\n",
       "      <td>-0.078864</td>\n",
       "      <td>0.122555</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.080489</td>\n",
       "      <td>0.182944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283319</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>-0.194443</td>\n",
       "      <td>0.162970</td>\n",
       "      <td>-0.021595</td>\n",
       "      <td>-0.174738</td>\n",
       "      <td>-0.022014</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.070484</td>\n",
       "      <td>0.097158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sclact</th>\n",
       "      <td>0.004323</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>-0.092375</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>0.128808</td>\n",
       "      <td>0.092673</td>\n",
       "      <td>0.192030</td>\n",
       "      <td>0.283319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>-0.059442</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>-0.004271</td>\n",
       "      <td>-0.038131</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>-0.107427</td>\n",
       "      <td>0.080589</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gndr</th>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>-0.029921</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>0.042046</td>\n",
       "      <td>-0.022413</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>0.024788</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.007807</td>\n",
       "      <td>-0.027735</td>\n",
       "      <td>-0.019075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agea</th>\n",
       "      <td>0.030051</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>0.257674</td>\n",
       "      <td>-0.029412</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.040351</td>\n",
       "      <td>-0.042970</td>\n",
       "      <td>-0.194443</td>\n",
       "      <td>-0.059442</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.256670</td>\n",
       "      <td>-0.015258</td>\n",
       "      <td>-0.033002</td>\n",
       "      <td>0.030541</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>-0.023270</td>\n",
       "      <td>0.048733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partner</th>\n",
       "      <td>-0.014931</td>\n",
       "      <td>0.018689</td>\n",
       "      <td>-0.028816</td>\n",
       "      <td>-0.034371</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>-0.022069</td>\n",
       "      <td>-0.145061</td>\n",
       "      <td>0.162970</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>-0.256670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019266</td>\n",
       "      <td>0.033158</td>\n",
       "      <td>-0.014865</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>-0.014130</td>\n",
       "      <td>0.004964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CH</th>\n",
       "      <td>-0.028832</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>-0.173825</td>\n",
       "      <td>0.031936</td>\n",
       "      <td>0.058250</td>\n",
       "      <td>0.079003</td>\n",
       "      <td>0.110593</td>\n",
       "      <td>-0.021595</td>\n",
       "      <td>-0.004271</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>-0.015258</td>\n",
       "      <td>-0.019266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.196084</td>\n",
       "      <td>-0.027113</td>\n",
       "      <td>-0.294179</td>\n",
       "      <td>-0.216024</td>\n",
       "      <td>-0.243774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CZ</th>\n",
       "      <td>-0.025586</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.218606</td>\n",
       "      <td>-0.225021</td>\n",
       "      <td>-0.215494</td>\n",
       "      <td>-0.166372</td>\n",
       "      <td>-0.206445</td>\n",
       "      <td>-0.174738</td>\n",
       "      <td>-0.038131</td>\n",
       "      <td>0.024788</td>\n",
       "      <td>-0.033002</td>\n",
       "      <td>0.033158</td>\n",
       "      <td>-0.196084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024048</td>\n",
       "      <td>-0.260926</td>\n",
       "      <td>-0.191605</td>\n",
       "      <td>-0.216219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>0.999965</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>-0.009180</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>-0.003884</td>\n",
       "      <td>-0.022014</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.030541</td>\n",
       "      <td>-0.014865</td>\n",
       "      <td>-0.027113</td>\n",
       "      <td>-0.024048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.036078</td>\n",
       "      <td>-0.026493</td>\n",
       "      <td>-0.029897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ES</th>\n",
       "      <td>-0.038364</td>\n",
       "      <td>-0.006935</td>\n",
       "      <td>0.035985</td>\n",
       "      <td>-0.152500</td>\n",
       "      <td>-0.188744</td>\n",
       "      <td>-0.223920</td>\n",
       "      <td>-0.074060</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>-0.107427</td>\n",
       "      <td>0.007807</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>-0.294179</td>\n",
       "      <td>-0.260926</td>\n",
       "      <td>-0.036078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.287460</td>\n",
       "      <td>-0.324387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NO</th>\n",
       "      <td>-0.018853</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>0.217530</td>\n",
       "      <td>0.191488</td>\n",
       "      <td>0.153183</td>\n",
       "      <td>0.100777</td>\n",
       "      <td>0.070484</td>\n",
       "      <td>0.080589</td>\n",
       "      <td>-0.027735</td>\n",
       "      <td>-0.023270</td>\n",
       "      <td>-0.014130</td>\n",
       "      <td>-0.216024</td>\n",
       "      <td>-0.191605</td>\n",
       "      <td>-0.026493</td>\n",
       "      <td>-0.287460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.238206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE</th>\n",
       "      <td>-0.031512</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>-0.050075</td>\n",
       "      <td>0.132684</td>\n",
       "      <td>0.161542</td>\n",
       "      <td>0.173921</td>\n",
       "      <td>0.063745</td>\n",
       "      <td>0.097158</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>-0.019075</td>\n",
       "      <td>0.048733</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>-0.243774</td>\n",
       "      <td>-0.216219</td>\n",
       "      <td>-0.029897</td>\n",
       "      <td>-0.324387</td>\n",
       "      <td>-0.238206</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             idno      year     tvtot   ppltrst   pplfair    pplhlp     happy  \\\n",
       "idno     1.000000  0.002102  0.003963 -0.007523  0.007397  0.004484 -0.003065   \n",
       "year     0.002102  1.000000 -0.031596 -0.001600  0.004283 -0.032901 -0.013378   \n",
       "tvtot    0.003963 -0.031596  1.000000 -0.142422 -0.119277 -0.069080 -0.118598   \n",
       "ppltrst -0.007523 -0.001600 -0.142422  1.000000  0.597506  0.459250  0.231533   \n",
       "pplfair  0.007397  0.004283 -0.119277  0.597506  1.000000  0.480931  0.247755   \n",
       "pplhlp   0.004484 -0.032901 -0.069080  0.459250  0.480931  1.000000  0.215323   \n",
       "happy   -0.003065 -0.013378 -0.118598  0.231533  0.247755  0.215323  1.000000   \n",
       "sclmeet -0.021376 -0.028502 -0.078864  0.122555  0.096501  0.080489  0.182944   \n",
       "sclact   0.004323  0.012150 -0.092375  0.137491  0.128808  0.092673  0.192030   \n",
       "gndr     0.010851  0.001355  0.017922 -0.029921  0.022251  0.042046 -0.022413   \n",
       "agea     0.030051 -0.003520  0.257674 -0.029412  0.014724  0.040351 -0.042970   \n",
       "partner -0.014931  0.018689 -0.028816 -0.034371 -0.034054 -0.022069 -0.145061   \n",
       "CH      -0.028832  0.003218 -0.173825  0.031936  0.058250  0.079003  0.110593   \n",
       "CZ      -0.025586  0.001472  0.218606 -0.225021 -0.215494 -0.166372 -0.206445   \n",
       "DE       0.999965  0.002100  0.004144 -0.009180  0.005904  0.003274 -0.003884   \n",
       "ES      -0.038364 -0.006935  0.035985 -0.152500 -0.188744 -0.223920 -0.074060   \n",
       "NO      -0.018853  0.001012 -0.017619  0.217530  0.191488  0.153183  0.100777   \n",
       "SE      -0.031512  0.002085 -0.050075  0.132684  0.161542  0.173921  0.063745   \n",
       "\n",
       "          sclmeet    sclact      gndr      agea   partner        CH        CZ  \\\n",
       "idno    -0.021376  0.004323  0.010851  0.030051 -0.014931 -0.028832 -0.025586   \n",
       "year    -0.028502  0.012150  0.001355 -0.003520  0.018689  0.003218  0.001472   \n",
       "tvtot   -0.078864 -0.092375  0.017922  0.257674 -0.028816 -0.173825  0.218606   \n",
       "ppltrst  0.122555  0.137491 -0.029921 -0.029412 -0.034371  0.031936 -0.225021   \n",
       "pplfair  0.096501  0.128808  0.022251  0.014724 -0.034054  0.058250 -0.215494   \n",
       "pplhlp   0.080489  0.092673  0.042046  0.040351 -0.022069  0.079003 -0.166372   \n",
       "happy    0.182944  0.192030 -0.022413 -0.042970 -0.145061  0.110593 -0.206445   \n",
       "sclmeet  1.000000  0.283319  0.009533 -0.194443  0.162970 -0.021595 -0.174738   \n",
       "sclact   0.283319  1.000000 -0.031648 -0.059442  0.011405 -0.004271 -0.038131   \n",
       "gndr     0.009533 -0.031648  1.000000  0.020598  0.033984  0.013925  0.024788   \n",
       "agea    -0.194443 -0.059442  0.020598  1.000000 -0.256670 -0.015258 -0.033002   \n",
       "partner  0.162970  0.011405  0.033984 -0.256670  1.000000 -0.019266  0.033158   \n",
       "CH      -0.021595 -0.004271  0.013925 -0.015258 -0.019266  1.000000 -0.196084   \n",
       "CZ      -0.174738 -0.038131  0.024788 -0.033002  0.033158 -0.196084  1.000000   \n",
       "DE      -0.022014  0.003764  0.011096  0.030541 -0.014865 -0.027113 -0.024048   \n",
       "ES       0.011601 -0.107427  0.007807  0.010583 -0.000387 -0.294179 -0.260926   \n",
       "NO       0.070484  0.080589 -0.027735 -0.023270 -0.014130 -0.216024 -0.191605   \n",
       "SE       0.097158  0.080040 -0.019075  0.048733  0.004964 -0.243774 -0.216219   \n",
       "\n",
       "               DE        ES        NO        SE  \n",
       "idno     0.999965 -0.038364 -0.018853 -0.031512  \n",
       "year     0.002100 -0.006935  0.001012  0.002085  \n",
       "tvtot    0.004144  0.035985 -0.017619 -0.050075  \n",
       "ppltrst -0.009180 -0.152500  0.217530  0.132684  \n",
       "pplfair  0.005904 -0.188744  0.191488  0.161542  \n",
       "pplhlp   0.003274 -0.223920  0.153183  0.173921  \n",
       "happy   -0.003884 -0.074060  0.100777  0.063745  \n",
       "sclmeet -0.022014  0.011601  0.070484  0.097158  \n",
       "sclact   0.003764 -0.107427  0.080589  0.080040  \n",
       "gndr     0.011096  0.007807 -0.027735 -0.019075  \n",
       "agea     0.030541  0.010583 -0.023270  0.048733  \n",
       "partner -0.014865 -0.000387 -0.014130  0.004964  \n",
       "CH      -0.027113 -0.294179 -0.216024 -0.243774  \n",
       "CZ      -0.024048 -0.260926 -0.191605 -0.216219  \n",
       "DE       1.000000 -0.036078 -0.026493 -0.029897  \n",
       "ES      -0.036078  1.000000 -0.287460 -0.324387  \n",
       "NO      -0.026493 -0.287460  1.000000 -0.238206  \n",
       "SE      -0.029897 -0.324387 -0.238206  1.000000  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['cntry'])], axis=1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the number of iterations to 1,000, the max depth to 4, the subsample to 0.2, and the learning rate\n",
      "to 0.25 just to see what happens. The percent errors in the training set are great, but this doesn't transfer\n",
      "to the test sets.\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.03505182760501909\n",
      "Percent Type II errors: 0.07337697763229678\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.147239263803681\n",
      "Percent Type II errors: 0.1607361963190184\n"
     ]
    }
   ],
   "source": [
    "# We'll make 1000 iterations, use 4-deep trees, and use all variables as features, use a 20% subsample,\n",
    "# and 0.25 learning rate.\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 4,\n",
    "          'loss': 'exponential',\n",
    "         'subsample':0.2,\n",
    "         'learning_rate':0.25}\n",
    "X = df\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Changing the number of iterations to 1,000, the max depth to 4, the subsample to 0.2, and the learning rate')\n",
    "print('to 0.25 just to see what happens. The percent errors in the training set are great, but this doesn\\'t transfer')\n",
    "print('to the test sets.\\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing the number of iterations to 2,000, the max depth to 4, the subsample to 0.2, and the learning rate\n",
      "to 0.5 just to see what happens. The percent errors in the training set are great, but this doesn't transfer\n",
      "to the test sets.\n",
      "\n",
      "It appears that increasing the learning rate decreases Type I errors but increase Type II errors and tends to overfit\n",
      "the training sets.\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.028096017457719585\n",
      "Percent Type II errors: 0.035870158210583744\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.19141104294478528\n",
      "Percent Type II errors: 0.1619631901840491\n"
     ]
    }
   ],
   "source": [
    "# We'll make 1000 iterations, use 4-deep trees, and use all variables as features, use a 20% subsample,\n",
    "# and 0.25 learning rate.\n",
    "params = {'n_estimators': 2000,\n",
    "          'max_depth': 4,\n",
    "          'loss': 'exponential',\n",
    "         'subsample':0.2,\n",
    "         'learning_rate':0.5}\n",
    "X = df\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Increasing the number of iterations to 2,000, the max depth to 4, the subsample to 0.2, and the learning rate')\n",
    "print('to 0.5 just to see what happens. The percent errors in the training set are great, but this doesn\\'t transfer')\n",
    "print('to the test sets.\\n')\n",
    "\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing the number of iterations to 2,000, the max depth to 4, the subsample to 0.2, and the learning rate\n",
      "to 0.5 just to see what happens. The percent errors in the training set are great, but this doesn't transfer\n",
      "to the test sets.\n",
      "\n",
      "It appears that increasing the learning rate Type I errors but increase Type II errors and tends to overfit\n",
      "the training sets.\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04446262956901255\n",
      "Percent Type II errors: 0.17444080741953083\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.0687116564417178\n",
      "Percent Type II errors: 0.18895705521472392\n"
     ]
    }
   ],
   "source": [
    "# We'll make 1000 iterations, use 4-deep trees, and use all variables as features, use a 20% subsample,\n",
    "# and 0.25 learning rate.\n",
    "params = {'n_estimators': 2000,\n",
    "          'max_depth': 3,\n",
    "          'loss': 'exponential',\n",
    "         'subsample':0.1,\n",
    "         'learning_rate':0.01}\n",
    "X = df\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Increasing the number of iterations to 2,000, the max depth to 4, the subsample to 0.2, and the learning rate')\n",
    "print('to 0.5 just to see what happens. The percent errors in the training set are great, but this doesn\\'t transfer')\n",
    "print('to the test sets.\\n')\n",
    "\n",
    "print('It appears that increasing the learning rate Type I errors but increase Type II errors and tends to overfit')\n",
    "print('the training sets.\\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing the number of iterations to 2,000, the max depth to 4, the subsample to 0.2, and the learning rate\n",
      "to 0.5 just to see what happens. The percent errors in the training set are great, but this doesn't transfer\n",
      "to the test sets.\n",
      "\n",
      "It appears that increasing the learning rate Type I errors but increase Type II errors and tends to overfit\n",
      "the training sets.\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04582651391162029\n",
      "Percent Type II errors: 0.17362247681396617\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06257668711656442\n",
      "Percent Type II errors: 0.18895705521472392\n"
     ]
    }
   ],
   "source": [
    "# We'll make 1000 iterations, use 4-deep trees, and use all variables as features, use a 20% subsample,\n",
    "# and 0.25 learning rate.\n",
    "params = {'n_estimators': 2000,\n",
    "          'max_depth': 3,\n",
    "          'loss': 'exponential',\n",
    "         'subsample':0.1,\n",
    "         'learning_rate':0.01}\n",
    "\n",
    "X = df.loc[:, ['happy', 'sclmeet', 'agea']]\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print('Increasing the number of iterations to 2,000, the max depth to 4, the subsample to 0.2, and the learning rate')\n",
    "print('to 0.5 just to see what happens. The percent errors in the training set are great, but this doesn\\'t transfer')\n",
    "print('to the test sets.\\n')\n",
    "\n",
    "print('It appears that increasing the learning rate Type I errors but increase Type II errors and tends to overfit')\n",
    "print('the training sets.\\n')\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best model I could come up with and it's no better than the original. Any feedback would be greatly appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
