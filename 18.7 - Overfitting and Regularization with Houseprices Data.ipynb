{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "from sqlalchemy import create_engine\n",
    "from scipy.stats import bartlett\n",
    "from scipy.stats import levene\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from statsmodels.tools.eval_measures import mse, rmse\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'houseprices'\n",
    "\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
    "houseprices = pd.read_sql_query('select * from houseprices',con=engine)\n",
    "\n",
    "# no need for an open connection, as we're only doing a single query\n",
    "engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original OLS/Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations in training set is 1168\n",
      "The number of observations in test set is 292\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              saleprice   R-squared:                       0.765\n",
      "Model:                            OLS   Adj. R-squared:                  0.764\n",
      "Method:                 Least Squares   F-statistic:                     630.2\n",
      "Date:                Sun, 03 Nov 2019   Prob (F-statistic):               0.00\n",
      "Time:                        11:21:45   Log-Likelihood:                -13980.\n",
      "No. Observations:                1168   AIC:                         2.797e+04\n",
      "Df Residuals:                    1161   BIC:                         2.801e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const             -4.502e+05   6.81e+04     -6.610      0.000   -5.84e+05   -3.17e+05\n",
      "overallqual        2.181e+04   1338.683     16.291      0.000    1.92e+04    2.44e+04\n",
      "totalbsmtsf          25.0559      3.092      8.104      0.000      18.990      31.122\n",
      "grade_living_area     3.5543      0.217     16.405      0.000       3.129       3.979\n",
      "garage_cars_area     53.0764      6.720      7.898      0.000      39.892      66.261\n",
      "TA                -9030.9603   2860.646     -3.157      0.002   -1.46e+04   -3418.346\n",
      "sold_remodeled        0.1056      0.018      5.830      0.000       0.070       0.141\n",
      "==============================================================================\n",
      "Omnibus:                      552.856   Durbin-Watson:                   1.870\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            60709.921\n",
      "Skew:                          -1.195   Prob(JB):                         0.00\n",
      "Kurtosis:                      38.239   Cond. No.                     2.38e+08\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.38e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "houseprices['garage_cars_area'] = houseprices['garagecars'] + houseprices['garagearea']\n",
    "houseprices['grade_living_area'] = houseprices.totrmsabvgrd  * houseprices.grlivarea\n",
    "houseprices = pd.concat([houseprices, pd.get_dummies(houseprices[\"kitchenqual\"])], axis=1)\n",
    "houseprices['sold_remodeled'] = houseprices.yearbuilt * houseprices.yearremodadd \n",
    "\n",
    "X = houseprices[['overallqual', 'totalbsmtsf', 'grade_living_area', 'garage_cars_area', 'TA', 'sold_remodeled']]\n",
    "Y = houseprices[['saleprice']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "print(\"The number of observations in training set is {}\".format(X_train.shape[0]))\n",
    "print(\"The number of observations in test set is {}\".format(Y_test.shape[0]))\n",
    "\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "# We fit an OLS model using statsmodels\n",
    "results = sm.OLS(Y_train, X_train_const).fit()\n",
    "\n",
    "# We print the summary results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('With Cross Validation:')\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Model\n",
    "\n",
    "It seems like a small lambda gives me the best R-squared value and lowest evaluation metrics for this model. My understanding is that this is because my OLS model was pretty decent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model on the training set is: 0.7650894698391388\n",
      "-----Test set statistics-----\n",
      "R-squared of the model on the test set is: 0.7747199235629062\n",
      "Mean absolute error of the prediction is: 25281.678001853765\n",
      "Mean squared error of the prediction is: 1512463768.1096895\n",
      "Root mean squared error of the prediction is: 38890.40714764618\n",
      "Mean absolute percentage error of the prediction is: 15.02318855855706\n"
     ]
    }
   ],
   "source": [
    "lassoregr = Lasso(alpha=.5) \n",
    "lassoregr.fit(X_train, Y_train)\n",
    "\n",
    "# We are making predictions here\n",
    "Y_preds_train = lassoregr.predict(X_train)\n",
    "Y_preds_test = lassoregr.predict(X_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(lassoregr.score(X_train, Y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(lassoregr.score(X_test, Y_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(Y_test, Y_preds_test)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mean_squared_error(Y_test, Y_preds_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(Y_test['saleprice'], Y_preds_test)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((Y_test['saleprice'] - Y_preds_test) / Y_test['saleprice'])) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Model\n",
    "\n",
    "Setting my lambda equal to zero gave me the highest R-squared values and lowest evaluation metrics, again indicating that my OLS model was a decent fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model on the training set is: 0.7650894701062251\n",
      "-----Test set statistics-----\n",
      "R-squared of the model on the test set is: 0.7747194916253897\n",
      "Mean absolute error of the prediction is: 25281.842278245338\n",
      "Mean squared error of the prediction is: [1.51246667e+09]\n",
      "Root mean squared error of the prediction is: [38890.44443061]\n",
      "Mean absolute percentage error of the prediction is: saleprice   15.023\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Fitting a ridge regression model. Alpha is the regularization\n",
    "# parameter (usually called lambda). As alpha gets larger, parameter\n",
    "# shrinkage grows more pronounced.\n",
    "ridgeregr = Ridge(alpha=0) \n",
    "ridgeregr.fit(X_train, Y_train)\n",
    "\n",
    "# We are making predictions here\n",
    "Y_preds_train = ridgeregr.predict(X_train)\n",
    "Y_preds_test = ridgeregr.predict(X_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(ridgeregr.score(X_train, Y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(ridgeregr.score(X_test, Y_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(Y_test, Y_preds_test)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(Y_test, Y_preds_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(Y_test, Y_preds_test)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((Y_test - Y_preds_test) / Y_test)) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regression Model\n",
    "\n",
    "Like the Ridge model, using 0 as my alpha gave me the best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model on the training set is: 0.765089470106225\n",
      "-----Test set statistics-----\n",
      "R-squared of the model on the test set is: 0.7747194916253897\n",
      "Mean absolute error of the prediction is: 25281.842278245324\n",
      "Mean squared error of the prediction is: 1512466668.010355\n",
      "Root mean squared error of the prediction is: 38890.44443060988\n",
      "Mean absolute percentage error of the prediction is: 15.023300986071774\n"
     ]
    }
   ],
   "source": [
    "elasticregr = ElasticNet(alpha=0, l1_ratio=0.5) \n",
    "elasticregr.fit(X_train, Y_train)\n",
    "\n",
    "# We are making predictions here\n",
    "Y_preds_train = elasticregr.predict(X_train)\n",
    "Y_preds_test = elasticregr.predict(X_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(elasticregr.score(X_train, Y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(elasticregr.score(X_test, Y_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(Y_test, Y_preds_test)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mean_squared_error(Y_test, Y_preds_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(sqrt(mean_squared_error(Y_test, Y_preds_test))))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((Y_test['saleprice'] - Y_preds_test)/Y_test['saleprice']) * 100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "My OLS model is a good fit for the data. I understand that because two of the other models gave the best vaues when the lambda/alpha was zero, the OLS model was not overfitting the data. \n",
    "\n",
    "Is there another way to choose a lambda without guess and check? It seems like cross-validation is the way to go, but I'm not sure I understand how that would work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
